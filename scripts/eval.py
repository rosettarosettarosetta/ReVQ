# ------------------------------------------------------------------------------
# ReVQ: Quantize-then-Rectify: Efficient VQ-VAE Training
# Copyright (c) 2025 Borui Zhang. All Rights Reserved.
# Licensed under the MIT License [see LICENSE for details]
# ------------------------------------------------------------------------------

#%%
import os
import sys
sys.path.append('..')
import math
from tqdm import trange, tqdm
import matplotlib.pyplot as plt
from omegaconf import OmegaConf
import argparse
import time, datetime
import logging
import torch
import torch.nn.functional as F
import torch.distributed as dist
import torch.nn as nn

from xvq.dataset import data_loader, load_preprocessor, load_frozen_vae, AudioPreprocessor
from xvq.utils import reconstruct_sample, seed_everything, check_rank_zero, to_ddp_model, set_train, set_eval
from xvq.models import setup_models
from xvq.evaluator import AudioEvaluator
from xvq.config import get_config

def log_and_print(message, logger=None):
    """åŒæ—¶è¾“å‡ºåˆ°loggerå’ŒæŽ§åˆ¶å°"""
    print(message)
    if logger is not None:
        logger.info(message)



def main_worker(rank, config):
    # setup devices
    torch.cuda.set_device(rank)
    device = torch.device(f"cuda:{rank}")
    config.rank = rank

    # setup distribution
    if config.world_size > 1:
        dist.init_process_group(
            backend=config.backend, init_method=config.init_method,
            rank=config.rank, world_size=config.world_size
        )

    if check_rank_zero():
        os.makedirs(config.log_path, exist_ok=True)

        logging.basicConfig(
            filename=os.path.join(config.log_path, "eval.log"),
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            filemode='a'  
        )

        logger = logging.getLogger(__name__)
        logger.info(" ReVQ eval Started")

 
    val_dataset, val_loader = data_loader(
        root_dir=config.data.data_val_path,
        split='val', 
        sample_rate= config.data.sample_rate, 
        acoustic_dim = config.data.acoustic_dim,
        win_size = config.data.win_size,
        hop_size = config.data.hop_size,
        batch_size=config.train.batch_size,
        num_workers=config.data.num_workers,
        shuffle=False
    )
    
    logger.info(f"Validation dataset size: {len(val_dataset)}")

    # ðŸ”¥ åˆ›å»ºéŸ³é¢‘é¢„å¤„ç†å™¨ï¼ˆä¸Žè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    audio_preprocessor = AudioPreprocessor(
        sample_rate=config.model.audio_preprocessor.sample_rate,
        n_fft=config.model.audio_preprocessor.n_fft,
        hop_length=config.model.audio_preprocessor.hop_length,
        n_mels=config.model.audio_preprocessor.n_mels,
        target_length=config.model.audio_preprocessor.target_length
    ).to(device)
    
    # setup the model
    decoder, quantizer, viewer = setup_models(config.model, device)
    vae_encoder, vae_decoder = load_frozen_vae(device=device, config=config.model, if_decoder=True)

    if config.world_size > 1:
        decoder = torch.nn.SyncBatchNorm.convert_sync_batchnorm(decoder)
    
    if check_rank_zero():
        get_param_num = lambda x: sum(p.numel() for p in x.parameters() if p.requires_grad)
        logger.info(f"Quantizer: {get_param_num(quantizer) / 1e6:.2f}M")
        logger.info(f"Decoder: {get_param_num(decoder) / 1e6:.2f}M")
        total_params = get_param_num(quantizer) + get_param_num(decoder)
        logger.info(f"Total params: {total_params / 1e6:.2f}M")
    
    # ðŸ”¥ åŠ è½½è®­ç»ƒå¥½çš„æ£€æŸ¥ç‚¹ï¼ˆä¸Žtrain.pyä¿å­˜æ ¼å¼ä¸€è‡´ï¼‰
    checkpoint_path = os.path.join(config.log_path, "ckpt.pth")
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
        quantizer.load_state_dict(checkpoint["quantizer"])
        decoder.load_state_dict(checkpoint["decoder"])
        audio_preprocessor.load_state_dict(checkpoint["audio_preprocessor"])
        if check_rank_zero():
            logger.info(f"âœ“ æ¨¡åž‹åŠ è½½å®Œæˆï¼Œæ¥è‡ªepoch {checkpoint['epoch']}")
            logger.info(f"  ä»Ž {checkpoint_path} åŠ è½½")
    else:
        if check_rank_zero():
            logger.info(f"âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    # start evaluation - åˆ›å»ºéŸ³é¢‘è¯„ä¼°å™¨
    quantizer, decoder = to_ddp_model(rank, quantizer, decoder)
    evaluator = AudioEvaluator(
        device=device,
        sample_rate=config.model.audio_preprocessor.sample_rate,
        n_fft=config.model.audio_preprocessor.n_fft,
        hop_length=config.model.audio_preprocessor.hop_length,
        n_mels=config.model.audio_preprocessor.n_mels,
        enable_time_domain=True,        # æ—¶åŸŸæŒ‡æ ‡ (MSE, MAE, SNR)
        enable_frequency_domain=True,   # é¢‘åŸŸæŒ‡æ ‡
        enable_mel_domain=True,         # MelåŸŸæŒ‡æ ‡
        enable_codebook_stats=True,     # ç æœ¬ç»Ÿè®¡
    )

    def audio_vq_infer(raw_audio):
        with torch.no_grad():
            data = audio_preprocessor(raw_audio)
            data = vae_encoder(data)
            data_shuffle = viewer.shuffle(data)
            quant_result = quantizer(data_shuffle)
            quant_shuffle = quant_result["x_quant"]
            quant = viewer.unshuffle(quant_shuffle)
            data_rec = decoder(quant)
            reconstructed_audio = vae_decoder(data_rec)
            
            return {
                'original_latent': data,
                'reconstructed_latent': data_rec,
                'quantized_latent': quant,
                'reconstructed_audio': reconstructed_audio,
                'codebook_indices': quant_result.get('indices', None)  # ç æœ¬ç´¢å¼•ï¼ˆå¦‚æžœæœ‰ï¼‰
            }
        
    set_eval(audio_preprocessor, vae_encoder, quantizer, decoder)  # è®¾ç½®æ‰€æœ‰æ¨¡åž‹ä¸ºè¯„ä¼°æ¨¡å¼
    pbar = tqdm(val_loader)     
    with torch.no_grad():
        total_loss = 0
        for i, (raw_audio, _) in enumerate(pbar):
            raw_audio = raw_audio.to(device)
            
            # ä½¿ç”¨å®Œæ•´çš„éŸ³é¢‘æŽ¨ç†æµç¨‹
            results = audio_vq_infer(raw_audio)
            
            # è®¡ç®—é‡å»ºæŸå¤±ï¼ˆä¸Žè®­ç»ƒéªŒè¯ä¸€è‡´ï¼‰
            loss_quant = F.mse_loss(
                results['original_latent'], 
                results['quantized_latent']
            )
            loss_decoder = F.mse_loss(
                results['original_latent'], 
                results['reconstructed_latent']
            )
            total_loss_batch = loss_quant + loss_decoder
            total_loss += total_loss_batch.item()
            
            # ðŸ”¥ æ›´æ–°éŸ³é¢‘è¯„ä¼°å™¨ï¼ˆä¼ å…¥åŽŸå§‹éŸ³é¢‘å’Œé‡å»ºéŸ³é¢‘ï¼‰
            evaluator.update(
                real_audio=raw_audio,
                fake_audio=results['reconstructed_audio'],
                codebook_indices=results['codebook_indices']
            )
            
            pbar.set_description(
                f"Eval {i}/{len(val_loader)} | "
                f"Loss: {total_loss_batch.item():.4f} | "
                f"Avg: {total_loss/(i+1):.4f}"
            )

            if (i+1) % 50 == 0:
                avg_loss = total_loss / (i+1)
                results_eval = evaluator.result()
                if check_rank_zero():
                    log_and_print(f"ðŸ“Š Progress: {i+1}/{len(val_loader)} batches, Average loss: {avg_loss:.4f}", logger)
                    log_and_print("ðŸŽµ å½“å‰éŸ³é¢‘è¯„ä¼°ç»“æžœ:", logger)
                    for key, value in results_eval.items():
                        log_and_print(f"  {key}: {value:.4f}", logger)

    # æœ€ç»ˆè¯„ä¼°ç»“æžœ
    final_avg_loss = total_loss / len(val_loader)
    final_results = evaluator.result()
    
    if check_rank_zero():
        log_and_print(f"ðŸŽ¯ è¯„ä¼°å®Œæˆ! æœ€ç»ˆç»“æžœ:", logger)
        log_and_print(f"  - Final average loss: {final_avg_loss:.4f}", logger)
        log_and_print(f"  - Processed {len(val_loader)} batches", logger)
        log_and_print(f"  - Total samples: {len(val_dataset)}", logger)
        log_and_print("", logger)
        log_and_print("ðŸŽµ æœ€ç»ˆéŸ³é¢‘è¯„ä¼°æŒ‡æ ‡:", logger)
        for key, value in final_results.items():
            log_and_print(f"  {key}: {value:.4f}", logger)
    else:
        print(f"Final average loss: {final_avg_loss:.4f}")
        print("Final audio evaluation results:")
        for key, value in final_results.items():
            print(f"  {key}: {value:.4f}")

    if dist.is_available() and dist.is_initialized():
        # destroy the process group
        dist.destroy_process_group()
    

def main():
    # setup config
    config = get_config()
    seed_everything(config.seed)
    
    # launch
    if config.world_size > 1:
        torch.multiprocessing.spawn(main_worker, args=(config,), nprocs=config.world_size)
    else:
        main_worker(0, config)
    
if __name__ == "__main__":
    main()
