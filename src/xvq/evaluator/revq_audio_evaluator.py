# ------------------------------------------------------------------------------
# ReVQ: Quantize-then-Rectify: Efficient VQ-VAE Training
# Copyright (c) 2025 Borui Zhang. All Rights Reserved.
# Licensed under the MIT License [see LICENSE for details]
# ------------------------------------------------------------------------------

import warnings
from typing import Optional, Mapping, Text
import numpy as np
import torch
import torch.nn.functional as F
import torchaudio


class AudioEvaluator:
    """é€šç”¨éŸ³é¢‘VQæ¨¡å‹è¯„ä¼°å™¨ï¼Œæ”¯æŒReVQç­‰å¤šç§éŸ³é¢‘é‡åŒ–æ¨¡å‹"""
    
    def __init__(
        self,
        device,
        sample_rate: int = 22050,
        n_fft: int = 1024,
        hop_length: int = 256,
        n_mels: int = 80,
        enable_time_domain: bool = True,      # æ—¶åŸŸæŒ‡æ ‡ (MSE, MAE, SNR)
        enable_frequency_domain: bool = True,  # é¢‘åŸŸæŒ‡æ ‡ (é¢‘è°±è·ç¦»)
        enable_mel_domain: bool = True,       # MelåŸŸæŒ‡æ ‡ (Melé¢‘è°±è·ç¦»)
        enable_codebook_stats: bool = True,   # ç æœ¬ç»Ÿè®¡
        num_codebook_entries: int = None,     # ç æœ¬å¤§å°ï¼ˆæ”¯æŒè‡ªåŠ¨æ£€æµ‹ï¼‰
        num_groups: int = None,               # åˆ†ç»„æ•°ï¼ˆReVQä¸“ç”¨ï¼Œæ”¯æŒè‡ªåŠ¨æ£€æµ‹ï¼‰
    ):
        """åˆå§‹åŒ–é€šç”¨éŸ³é¢‘è¯„ä¼°å™¨
        
        Args:
            device: è®¡ç®—è®¾å¤‡
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
            n_fft: FFTçª—å£å¤§å°
            hop_length: FFTè·³è·ƒé•¿åº¦  
            n_mels: Melæ»¤æ³¢å™¨æ•°é‡
            enable_time_domain: æ˜¯å¦å¯ç”¨æ—¶åŸŸè¯„ä¼°
            enable_frequency_domain: æ˜¯å¦å¯ç”¨é¢‘åŸŸè¯„ä¼°
            enable_mel_domain: æ˜¯å¦å¯ç”¨MelåŸŸè¯„ä¼°
            enable_codebook_stats: æ˜¯å¦å¯ç”¨ç æœ¬ç»Ÿè®¡
            num_codebook_entries: æ€»ç æœ¬æ¡ç›®æ•°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
            num_groups: åˆ†ç»„æ•°é‡ï¼ˆReVQä¸“ç”¨ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        self._device = device
        self._sample_rate = sample_rate
        self._n_fft = n_fft
        self._hop_length = hop_length
        self._n_mels = n_mels
        
        # åŠŸèƒ½å¼€å…³
        self._enable_time_domain = enable_time_domain
        self._enable_frequency_domain = enable_frequency_domain
        self._enable_mel_domain = enable_mel_domain
        self._enable_codebook_stats = enable_codebook_stats
        
        # ç æœ¬å‚æ•°ï¼ˆæ”¯æŒè‡ªåŠ¨æ£€æµ‹ï¼‰
        self._num_codebook_entries = num_codebook_entries
        self._num_groups = num_groups
        self._auto_detect_codebook = (num_codebook_entries is None) or (num_groups is None)
        
        # å¦‚æœæä¾›äº†å®Œæ•´å‚æ•°ï¼Œè®¡ç®—æ¯ç»„ç å­—æ•°
        if self._num_codebook_entries is not None and self._num_groups is not None:
            self._codes_per_group = self._num_codebook_entries // self._num_groups
        
        # åˆå§‹åŒ–Melé¢‘è°±å˜æ¢
        if self._enable_mel_domain:
            self._mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=sample_rate,
                n_fft=n_fft,
                hop_length=hop_length,
                n_mels=n_mels,
                power=2.0,
                normalized=True
            ).to(device)
        
        self.reset_metrics()
    
    def reset_metrics(self):
        """é‡ç½®æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        self._num_examples = 0
        self._num_updates = 0
        
        # æ—¶åŸŸæŒ‡æ ‡
        self._mse_total = 0.0
        self._mae_total = 0.0
        self._snr_total = 0.0
        
        # é¢‘åŸŸæŒ‡æ ‡
        self._spectral_mse_total = 0.0
        self._spectral_mae_total = 0.0
        
        # MelåŸŸæŒ‡æ ‡
        self._mel_mse_total = 0.0
        self._mel_mae_total = 0.0
        
        # ç æœ¬ç»Ÿè®¡ï¼ˆåªæœ‰åœ¨ç¡®å®šå‚æ•°åæ‰åˆå§‹åŒ–ï¼‰
        if self._enable_codebook_stats and not self._auto_detect_codebook:
            self._init_codebook_stats()
    
    def _init_codebook_stats(self):
        """åˆå§‹åŒ–ç æœ¬ç»Ÿè®¡"""
        # å…¨å±€ç æœ¬ä½¿ç”¨æƒ…å†µ
        self._used_codes = set()
        # æ¯ç»„ç æœ¬ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœæœ‰åˆ†ç»„ï¼‰
        if self._num_groups is not None:
            self._group_used_codes = [set() for _ in range(self._num_groups)]
        # ç æœ¬é¢‘ç‡ç»Ÿè®¡
        if self._num_codebook_entries is not None:
            self._code_frequencies = torch.zeros(
                self._num_codebook_entries, 
                dtype=torch.float64, 
                device=self._device
            )
    
    def _auto_detect_codebook_structure(self, codebook_indices: torch.Tensor):
        """è‡ªåŠ¨æ£€æµ‹ç æœ¬ç»“æ„"""
        if not self._auto_detect_codebook:
            return
        
        print(f"ğŸ” æ­£åœ¨è‡ªåŠ¨æ£€æµ‹ç æœ¬ç»“æ„...")
        
        # æ£€æµ‹ç æœ¬å¤§å°
        if self._num_codebook_entries is None:
            max_index = codebook_indices.max().item()
            self._num_codebook_entries = max_index + 1
            print(f"  - æ£€æµ‹åˆ°ç æœ¬å¤§å°: {self._num_codebook_entries}")
        
        # æ£€æµ‹åˆ†ç»„ç»“æ„ï¼ˆReVQç‰¹æœ‰ï¼‰
        if len(codebook_indices.shape) >= 3 and self._num_groups is None:
            self._num_groups = codebook_indices.shape[1]  # ç¬¬äºŒç»´é€šå¸¸æ˜¯ç»„æ•°
            if self._num_codebook_entries is not None:
                self._codes_per_group = self._num_codebook_entries // self._num_groups
                print(f"  - æ£€æµ‹åˆ°ReVQç»“æ„: {self._num_groups}ç»„ï¼Œæ¯ç»„{self._codes_per_group}ä¸ªç å­—")
        
        # åˆå§‹åŒ–ç æœ¬ç»Ÿè®¡
        if self._enable_codebook_stats:
            self._init_codebook_stats()
            print(f"  - ç æœ¬ç»Ÿè®¡å·²åˆå§‹åŒ–")
        
        self._auto_detect_codebook = False  # åªæ£€æµ‹ä¸€æ¬¡
    
    def _compute_snr(self, signal: torch.Tensor, noise: torch.Tensor) -> float:
        """è®¡ç®—ä¿¡å™ªæ¯” (SNR)"""
        signal_power = torch.mean(signal ** 2)
        noise_power = torch.mean(noise ** 2)
        
        if noise_power == 0:
            return float('inf')
        
        snr = 10 * torch.log10(signal_power / noise_power)
        return snr.item()
    
    def _compute_spectral_metrics(self, real_audio: torch.Tensor, fake_audio: torch.Tensor):
        """è®¡ç®—é¢‘è°±åŸŸæŒ‡æ ‡"""
        # è®¡ç®—STFT
        real_stft = torch.stft(
            real_audio.squeeze(), 
            n_fft=self._n_fft, 
            hop_length=self._hop_length,
            window=torch.hann_window(self._n_fft).to(self._device),
            return_complex=True
        )
        fake_stft = torch.stft(
            fake_audio.squeeze(), 
            n_fft=self._n_fft, 
            hop_length=self._hop_length,
            window=torch.hann_window(self._n_fft).to(self._device),
            return_complex=True
        )
        
        # è®¡ç®—å¹…åº¦è°±
        real_mag = torch.abs(real_stft)
        fake_mag = torch.abs(fake_stft)
        
        # é¢‘è°±MSEå’ŒMAE
        spec_mse = F.mse_loss(real_mag, fake_mag).item()
        spec_mae = F.l1_loss(real_mag, fake_mag).item()
        
        return spec_mse, spec_mae
    
    def _compute_mel_metrics(self, real_audio: torch.Tensor, fake_audio: torch.Tensor):
        """è®¡ç®—MelåŸŸæŒ‡æ ‡"""
        # ç¡®ä¿éŸ³é¢‘ç»´åº¦æ­£ç¡®
        if real_audio.dim() == 1:
            real_audio = real_audio.unsqueeze(0)
        if fake_audio.dim() == 1:
            fake_audio = fake_audio.unsqueeze(0)
        
        # è®¡ç®—Melé¢‘è°±
        real_mel = self._mel_transform(real_audio)
        fake_mel = self._mel_transform(fake_audio)
        
        # è½¬æ¢ä¸ºå¯¹æ•°åˆ»åº¦ï¼ˆdBï¼‰
        real_mel_db = torchaudio.transforms.AmplitudeToDB()(real_mel)
        fake_mel_db = torchaudio.transforms.AmplitudeToDB()(fake_mel)
        
        # Melé¢‘è°±MSEå’ŒMAE
        mel_mse = F.mse_loss(real_mel_db, fake_mel_db).item()
        mel_mae = F.l1_loss(real_mel_db, fake_mel_db).item()
        
        return mel_mse, mel_mae
    
    def _update_codebook_stats(self, codebook_indices: torch.Tensor):
        """æ›´æ–°ç æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        if not self._enable_codebook_stats:
            return
        
        # å¦‚æœè¿˜æ²¡æ£€æµ‹è¿‡ç æœ¬ç»“æ„ï¼Œå…ˆè¿›è¡Œæ£€æµ‹
        if self._auto_detect_codebook:
            self._auto_detect_codebook_structure(codebook_indices)
        
        # codebook_indiceså¯èƒ½æœ‰ä¸åŒçš„å½¢çŠ¶
        # ReVQ: [batch_size, groups, spatial_dims...]
        # å…¶ä»–VQ: [batch_size, spatial_dims...]
        batch_size = codebook_indices.shape[0]
        
        for b in range(batch_size):
            indices_batch = codebook_indices[b]
            
            if self._num_groups is not None and len(indices_batch.shape) >= 2:
                # ReVQåˆ†ç»„æƒ…å†µ
                for g in range(min(self._num_groups, indices_batch.shape[0])):
                    group_indices = indices_batch[g].flatten()
                    
                    # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
                    if hasattr(self, '_codes_per_group'):
                        global_indices = g * self._codes_per_group + group_indices
                    else:
                        global_indices = group_indices
                    
                    # æ›´æ–°ç»Ÿè®¡
                    unique_indices = torch.unique(global_indices)
                    if hasattr(self, '_used_codes'):
                        self._used_codes.update(unique_indices.cpu().tolist())
                    if hasattr(self, '_group_used_codes') and g < len(self._group_used_codes):
                        self._group_used_codes[g].update(group_indices.cpu().tolist())
                    
                    # æ›´æ–°é¢‘ç‡ç»Ÿè®¡
                    if hasattr(self, '_code_frequencies'):
                        for idx in global_indices:
                            if 0 <= idx < self._code_frequencies.shape[0]:
                                self._code_frequencies[idx] += 1
            else:
                # ä¼ ç»ŸVQæƒ…å†µ
                all_indices = indices_batch.flatten()
                unique_indices = torch.unique(all_indices)
                
                if hasattr(self, '_used_codes'):
                    self._used_codes.update(unique_indices.cpu().tolist())
                
                if hasattr(self, '_code_frequencies'):
                    for idx in all_indices:
                        if 0 <= idx < self._code_frequencies.shape[0]:
                            self._code_frequencies[idx] += 1
    
    def update(
        self,
        real_audio: torch.Tensor,
        fake_audio: torch.Tensor,
        codebook_indices: Optional[torch.Tensor] = None
    ):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡
        
        Args:
            real_audio: åŸå§‹éŸ³é¢‘ [batch_size, time] æˆ– [batch_size, channels, time]
            fake_audio: é‡å»ºéŸ³é¢‘ [batch_size, time] æˆ– [batch_size, channels, time]  
            codebook_indices: ç æœ¬ç´¢å¼• [batch_size, groups, spatial_dims...]
        """
        batch_size = real_audio.shape[0]
        self._num_examples += batch_size
        self._num_updates += 1
        
        # ç¡®ä¿éŸ³é¢‘åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        real_audio = real_audio.to(self._device)
        fake_audio = fake_audio.to(self._device)
        
        # å¤„ç†æ¯ä¸ªéŸ³é¢‘æ ·æœ¬
        for i in range(batch_size):
            real_sample = real_audio[i]
            fake_sample = fake_audio[i]
            
            # å¦‚æœæ˜¯å¤šå£°é“ï¼Œå–å¹³å‡æˆ–ç¬¬ä¸€ä¸ªå£°é“
            if real_sample.dim() > 1:
                real_sample = real_sample.mean(dim=0)
            if fake_sample.dim() > 1:
                fake_sample = fake_sample.mean(dim=0)
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(real_sample.shape[-1], fake_sample.shape[-1])
            real_sample = real_sample[..., :min_len]
            fake_sample = fake_sample[..., :min_len]
            
            # æ—¶åŸŸæŒ‡æ ‡
            if self._enable_time_domain:
                # MSEå’ŒMAE
                mse = F.mse_loss(real_sample, fake_sample).item()
                mae = F.l1_loss(real_sample, fake_sample).item()
                self._mse_total += mse
                self._mae_total += mae
                
                # SNR
                noise = real_sample - fake_sample
                snr = self._compute_snr(real_sample, noise)
                if not np.isinf(snr):
                    self._snr_total += snr
            
            # é¢‘åŸŸæŒ‡æ ‡
            if self._enable_frequency_domain:
                spec_mse, spec_mae = self._compute_spectral_metrics(real_sample, fake_sample)
                self._spectral_mse_total += spec_mse
                self._spectral_mae_total += spec_mae
            
            # MelåŸŸæŒ‡æ ‡
            if self._enable_mel_domain:
                mel_mse, mel_mae = self._compute_mel_metrics(real_sample, fake_sample)
                self._mel_mse_total += mel_mse
                self._mel_mae_total += mel_mae
        
        # ç æœ¬ç»Ÿè®¡
        if codebook_indices is not None:
            self._update_codebook_stats(codebook_indices)
    
    def result(self) -> Mapping[Text, float]:
        """è¿”å›è¯„ä¼°ç»“æœ"""
        if self._num_examples < 1:
            raise ValueError("No examples to evaluate.")
        
        results = {}
        
        # æ—¶åŸŸæŒ‡æ ‡
        if self._enable_time_domain:
            results["Time_MSE"] = self._mse_total / self._num_examples
            results["Time_MAE"] = self._mae_total / self._num_examples
            results["Time_RMSE"] = np.sqrt(results["Time_MSE"])
            results["SNR_dB"] = self._snr_total / self._num_examples
        
        # é¢‘åŸŸæŒ‡æ ‡
        if self._enable_frequency_domain:
            results["Spectral_MSE"] = self._spectral_mse_total / self._num_examples
            results["Spectral_MAE"] = self._spectral_mae_total / self._num_examples
        
        # MelåŸŸæŒ‡æ ‡
        if self._enable_mel_domain:
            results["Mel_MSE"] = self._mel_mse_total / self._num_examples
            results["Mel_MAE"] = self._mel_mae_total / self._num_examples
        
        # ç æœ¬ç»Ÿè®¡
        if self._enable_codebook_stats and hasattr(self, '_used_codes'):
            # å…¨å±€ç æœ¬ä½¿ç”¨ç‡
            if self._num_codebook_entries is not None:
                total_usage = len(self._used_codes) / self._num_codebook_entries
                results["Codebook_Usage"] = total_usage
            
            # åˆ†ç»„ç»Ÿè®¡ï¼ˆReVQç‰¹æœ‰ï¼‰
            if hasattr(self, '_group_used_codes') and hasattr(self, '_codes_per_group'):
                group_usages = []
                for g in range(len(self._group_used_codes)):
                    group_usage = len(self._group_used_codes[g]) / self._codes_per_group
                    group_usages.append(group_usage)
                
                if group_usages:
                    results["Avg_Group_Usage"] = np.mean(group_usages)
                    results["Min_Group_Usage"] = np.min(group_usages)
                    results["Max_Group_Usage"] = np.max(group_usages)
            
            # ç æœ¬ç†µ
            if hasattr(self, '_code_frequencies') and self._code_frequencies.sum() > 0:
                probs = self._code_frequencies / self._code_frequencies.sum()
                entropy = (-torch.log2(probs + 1e-12) * probs).sum().item()
                results["Codebook_Entropy"] = entropy
                
                # å½’ä¸€åŒ–ç†µ
                if self._num_codebook_entries is not None:
                    max_entropy = np.log2(self._num_codebook_entries)
                    results["Normalized_Entropy"] = entropy / max_entropy
            else:
                results["Codebook_Entropy"] = 0.0
                if self._num_codebook_entries is not None:
                    results["Normalized_Entropy"] = 0.0
        
        return results


if __name__ == "__main__":
    # æµ‹è¯•é€šç”¨éŸ³é¢‘è¯„ä¼°å™¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("ğŸµ æµ‹è¯•é€šç”¨éŸ³é¢‘è¯„ä¼°å™¨...")
    
    # æµ‹è¯•1: ReVQæ¨¡å¼ï¼ˆæä¾›å®Œæ•´å‚æ•°ï¼‰
    print("\n1. ReVQæ¨¡å¼æµ‹è¯•:")
    evaluator_revq = AudioEvaluator(
        device=device,
        sample_rate=22050,
        num_codebook_entries=65536,
        num_groups=256
    )
    
    # ç”ŸæˆReVQæµ‹è¯•æ•°æ®
    batch_size = 2
    audio_length = 22050  # 1ç§’
    groups = 256
    spatial_dim = 32
    
    real_audio = torch.randn(batch_size, audio_length).to(device)
    fake_audio = real_audio + torch.randn_like(real_audio) * 0.1
    revq_indices = torch.randint(0, 256, (batch_size, groups, spatial_dim)).to(device)
    
    evaluator_revq.update(real_audio, fake_audio, revq_indices)
    results_revq = evaluator_revq.result()
    
    print("ReVQè¯„ä¼°ç»“æœ:")
    for metric, value in results_revq.items():
        if isinstance(value, float):
            print(f"  {metric}: {value:.4f}")
    
    # æµ‹è¯•2: è‡ªåŠ¨æ£€æµ‹æ¨¡å¼
    print("\n2. è‡ªåŠ¨æ£€æµ‹æ¨¡å¼æµ‹è¯•:")
    evaluator_auto = AudioEvaluator(
        device=device,
        sample_rate=22050,
        # ä¸æä¾›ç æœ¬å‚æ•°ï¼Œè®©å®ƒè‡ªåŠ¨æ£€æµ‹
    )
    
    # ç”Ÿæˆä¼ ç»ŸVQæµ‹è¯•æ•°æ®
    vq_indices = torch.randint(0, 1024, (batch_size, 64, 64)).to(device)
    
    evaluator_auto.update(real_audio, fake_audio, vq_indices)
    results_auto = evaluator_auto.result()
    
    print("è‡ªåŠ¨æ£€æµ‹è¯„ä¼°ç»“æœ:")
    for metric, value in results_auto.items():
        if isinstance(value, float):
            print(f"  {metric}: {value:.4f}")
    
    # æµ‹è¯•3: ä»…éŸ³é¢‘è´¨é‡è¯„ä¼°ï¼ˆæ— ç æœ¬ç»Ÿè®¡ï¼‰
    print("\n3. çº¯éŸ³é¢‘è´¨é‡è¯„ä¼°:")
    evaluator_audio_only = AudioEvaluator(
        device=device,
        sample_rate=22050,
        enable_codebook_stats=False  # å…³é—­ç æœ¬ç»Ÿè®¡
    )
    
    evaluator_audio_only.update(real_audio, fake_audio)  # ä¸ä¼ ç æœ¬ç´¢å¼•
    results_audio_only = evaluator_audio_only.result()
    
    print("çº¯éŸ³é¢‘è´¨é‡è¯„ä¼°ç»“æœ:")
    for metric, value in results_audio_only.items():
        if isinstance(value, float):
            print(f"  {metric}: {value:.4f}")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
